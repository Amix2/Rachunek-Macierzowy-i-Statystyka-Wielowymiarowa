---
title: "Zastosowanie selekcji cech i regularyzacji"
date: '2022'
output: html_document
autor: Maciej Mucha, Łukasz Wroński
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(MASS)
library(ISLR)
library(class)
library(leaps)

```

Autorzy: Maciej Mucha, Łukasz Wroński

Import danych. Źródło: <https://www.kaggle.com/datasets/paradisejoy/top-hits-spotify-from-20002019>

```{r import}
Data = read.csv("data.csv")
Data <- na.omit(Data)
Data = Data[,3:17]
Data$explicit <- as.integer(Data$explicit == "True")
```

Wybór najepszego podzbioru

```{r}
songs_bs <- regsubsets(loudness ~ ., data = Data, really.big = TRUE, nvmax=15)
songs_bs_sum <- summary(songs_bs)
songs_bs_sum
```

```{r}
songs_bs_sum$cp
songs_bs_sum$bic
```

```{r}
bic_min <- which.min(songs_bs_sum$bic)
bic_min
songs_bs_sum$bic[bic_min]
```

```{r}
plot(songs_bs_sum$bic, xlab = "Liczba zmiennych", ylab = "BIC", col = "green",
     type = "b", pch = 20)
points(bic_min, songs_bs_sum$bic[bic_min], col = "red", pch = 9)

```

```{r}
plot(songs_bs, scale = "bic")
```

Najbardziej istotne predykaty to:

-   year
-   energy
-   speechiness
-   instrumentalness

# Selekcja krokowa do przodu i wstecz

```{r}
songs_fwd <- regsubsets(loudness ~ ., data = Data, really.big = TRUE, nvmax=15,
                          method = "forward")
songs_fwd_sum <- summary(songs_fwd)
songs_fwd_sum
songs_back <- regsubsets(loudness ~ ., data = Data, really.big = TRUE, nvmax=15,
                           method = "backward")
songs_back_sum <- summary(songs_back)
songs_back_sum
```

Wykres dla selekcji krokowej do przodu

```{r}
plot(songs_fwd, scale = "bic")
```

Wykres dla selekcji krokowej do tyłu

```{r}
plot(songs_back, scale = "bic")
```

## LASSO
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(ISLR)
library(glmnet)
Data = read.csv("data.csv")
Data = Data[,3:17]
Data <- na.omit(Data)
```
Tworzenie macierzy X
```{r modelmatrix}
X <- model.matrix(Data$popularity ~ ., data=Data)[, -1]
y <- Data$popularity
```

```{r}
set.seed(2)
n <- nrow(X)
train <- sample(n, n / 2)
test <- -train
```

Dopasowujemy lasso dla ustalonej siatki parametrów regularyzacji
```{r}
fit_lasso <- glmnet(X[train,], y[train], alpha = 1)
plot(fit_lasso, xvar = "lambda")
```

```{r}
cv_out <- cv.glmnet(X[train,], y[train], alpha = 1)
plot(cv_out)
cv_out$lambda.min
pred_lasso <- predict(fit_lasso, s = cv_out$lambda.min, newx = X[test,])
mean((pred_lasso - y[test])^2)
```


```{r}
predict(fit_lasso, s = cv_out$lambda.min, newx = X[test,], type = "coefficients")
```

Najlepsza okazaa się kompinacja 4 predyntkóółłłłąąąrów: duration_ms, loudness,ółółółółąż acousticness, instrumentalness                       